#!/usr/bin/env python3
"""
æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½
éªŒè¯åŠ¨æ€å›¾è¡¨ç”Ÿæˆå’Œæ•°æ®æ”¶é›†åŠŸèƒ½
"""

import os
import sys
import numpy as np
import time
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.utils import load_config
from src.visualization import TrainingVisualizer, EnhancedMetricsCollector


def test_visualization():
    """æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½...")
    
    # åŠ è½½é…ç½®
    config = load_config('config.yaml')
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = Path("test_plots")
    test_dir.mkdir(exist_ok=True)
    
    # åˆå§‹åŒ–å¯è§†åŒ–å™¨
    visualizer = TrainingVisualizer(config, save_dir=str(test_dir))
    
    # ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...")
    
    num_episodes = 200
    for episode in range(1, num_episodes + 1):
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡å˜åŒ–
        
        # å¥–åŠ±é€æ¸æå‡ï¼ˆå¸¦å™ªå£°ï¼‰
        base_reward = -50 + (episode / num_episodes) * 100
        reward = base_reward + np.random.normal(0, 10)
        
        # æŸå¤±é€æ¸ä¸‹é™
        actor_loss = 2.0 * np.exp(-episode / 50) + np.random.normal(0, 0.1)
        critic_loss = 1.5 * np.exp(-episode / 40) + np.random.normal(0, 0.08)
        
        # ç†µé€æ¸ä¸‹é™
        entropy = 1.0 * np.exp(-episode / 80) + 0.1 + np.random.normal(0, 0.05)
        
        # å®Œæˆç‡é€æ¸æå‡
        completion_rate = min(1.0, (episode / num_episodes) * 1.2 + np.random.normal(0, 0.1))
        completion_rate = max(0.0, completion_rate)
        
        # è½½é‡åˆ©ç”¨ç‡é€æ¸æå‡
        load_utilization = min(1.0, (episode / num_episodes) * 0.8 + np.random.normal(0, 0.05))
        load_utilization = max(0.0, load_utilization)
        
        # è·¯å¾„é•¿åº¦é€æ¸ä¼˜åŒ–
        path_length = 100 - (episode / num_episodes) * 30 + np.random.normal(0, 5)
        path_length = max(20, path_length)
        
        # ç¢°æ’æ¬¡æ•°é€æ¸å‡å°‘
        collision_count = max(0, int(10 * np.exp(-episode / 30) + np.random.poisson(1)))
        
        # å›åˆé•¿åº¦
        episode_length = int(200 + np.random.normal(0, 20))
        
        # æ·»åŠ æ•°æ®åˆ°å¯è§†åŒ–å™¨
        visualizer.add_episode_data(
            episode=episode,
            reward=reward,
            actor_loss=actor_loss,
            critic_loss=critic_loss,
            entropy=entropy,
            completion_rate=completion_rate,
            load_utilization=load_utilization,
            path_length=path_length,
            collision_count=collision_count,
            episode_length=episode_length
        )
        
        # æ¯20ä¸ªepisodeæ›´æ–°ä¸€æ¬¡å›¾è¡¨
        if episode % 20 == 0:
            print(f"  Episode {episode}: å¥–åŠ±={reward:.2f}, å®Œæˆç‡={completion_rate:.2f}")
    
    print("âœ… æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # åˆ›å»ºè®­ç»ƒå›¾è¡¨
    print("ğŸ¨ ç”Ÿæˆè®­ç»ƒå›¾è¡¨...")
    fig = visualizer.create_training_plots()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = test_dir / "test_training_plots.png"
    fig.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
    
    # ä¿å­˜æ•°æ®
    data_path = test_dir / "test_training_data.json"
    visualizer.save_data_to_json(str(data_path))
    print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {data_path}")
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    report_path = test_dir / "test_training_report.json"
    report = visualizer.create_summary_report(str(report_path))
    print(f"ğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
    if report:
        summary = report['training_summary']
        print("\nğŸ“‹ è®­ç»ƒæ€»ç»“:")
        print(f"  æ€»å›åˆæ•°: {summary['total_episodes']}")
        print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {summary['final_performance']['avg_reward_last_100']:.2f}")
        print(f"  æœ€ç»ˆå®Œæˆç‡: {summary['final_performance']['avg_completion_rate_last_100']:.2f}")
        print(f"  æœ€ä½³å¥–åŠ±: {summary['final_performance']['best_reward']:.2f}")
        print(f"  å¥–åŠ±æå‡: {summary['learning_progress']['reward_improvement']:.2f}")
    
    print("âœ… å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å®Œæˆ")


def test_metrics_collector():
    """æµ‹è¯•æŒ‡æ ‡æ”¶é›†å™¨"""
    print("\nğŸ§ª æµ‹è¯•æŒ‡æ ‡æ”¶é›†å™¨...")
    
    config = load_config('config.yaml')
    collector = EnhancedMetricsCollector(config)
    
    # æ¨¡æ‹Ÿç¯å¢ƒä¿¡æ¯
    class MockEnv:
        def __init__(self):
            self.max_load = 25
    
    mock_env = MockEnv()
    
    # æ”¶é›†ä¸€äº›æ¨¡æ‹ŸæŒ‡æ ‡
    for episode in range(1, 21):
        episode_reward = -20 + episode * 2 + np.random.normal(0, 5)
        episode_length = 150 + np.random.randint(-20, 20)
        
        # æ¨¡æ‹Ÿç¯å¢ƒä¿¡æ¯
        info = {
            'completion_rate': min(1.0, episode / 20 + np.random.normal(0, 0.1)),
            'agv_states': [(0, 0, np.random.uniform(0, 25)) for _ in range(3)],
            'episode_stats': {
                'collisions': np.random.randint(0, 5),
                'deadlocks': np.random.randint(0, 2)
            }
        }
        
        # æ”¶é›†å›åˆæŒ‡æ ‡
        metrics = collector.collect_episode_metrics(
            mock_env, episode, episode_reward, episode_length, info
        )
        
        print(f"  Episode {episode}: å¥–åŠ±={metrics['episode_reward']:.2f}, "
              f"å®Œæˆç‡={metrics['completion_rate']:.2f}, "
              f"è½½é‡åˆ©ç”¨ç‡={metrics['load_utilization']:.2f}")
    
    # è·å–æœ€è¿‘æŒ‡æ ‡
    recent_metrics = collector.get_recent_metrics(10)
    print(f"\nğŸ“Š æœ€è¿‘10å›åˆå¹³å‡æŒ‡æ ‡:")
    for key, value in recent_metrics.items():
        print(f"  {key}: {value:.3f}")
    
    print("âœ… æŒ‡æ ‡æ”¶é›†å™¨æµ‹è¯•å®Œæˆ")


def test_realtime_visualization():
    """æµ‹è¯•å®æ—¶å¯è§†åŒ–"""
    print("\nğŸ§ª æµ‹è¯•å®æ—¶å¯è§†åŒ–...")
    
    config = load_config('config.yaml')
    test_dir = Path("test_realtime")
    test_dir.mkdir(exist_ok=True)
    
    visualizer = TrainingVisualizer(config, save_dir=str(test_dir))
    
    # å¯åŠ¨å®æ—¶å¯è§†åŒ–
    visualizer.start_realtime_visualization(update_frequency=5)
    
    print("ğŸ”„ å¼€å§‹å®æ—¶æ•°æ®æ›´æ–°...")
    
    # æ¨¡æ‹Ÿå®æ—¶è®­ç»ƒæ•°æ®
    for episode in range(1, 31):
        reward = -30 + episode + np.random.normal(0, 5)
        actor_loss = 1.0 * np.exp(-episode / 10) + np.random.normal(0, 0.1)
        critic_loss = 0.8 * np.exp(-episode / 8) + np.random.normal(0, 0.05)
        entropy = 0.5 * np.exp(-episode / 15) + 0.1 + np.random.normal(0, 0.02)
        completion_rate = min(1.0, episode / 30 + np.random.normal(0, 0.05))
        
        visualizer.add_episode_data(
            episode=episode,
            reward=reward,
            actor_loss=actor_loss,
            critic_loss=critic_loss,
            entropy=entropy,
            completion_rate=completion_rate
        )
        
        print(f"  æ·»åŠ Episode {episode}æ•°æ®")
        time.sleep(0.1)  # æ¨¡æ‹Ÿè®­ç»ƒé—´éš”
    
    # ç­‰å¾…æœ€åä¸€æ¬¡æ›´æ–°
    time.sleep(2)
    
    # åœæ­¢å®æ—¶å¯è§†åŒ–
    visualizer.stop_realtime_visualization()
    
    print("âœ… å®æ—¶å¯è§†åŒ–æµ‹è¯•å®Œæˆ")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        # æµ‹è¯•åŸºç¡€å¯è§†åŒ–åŠŸèƒ½
        test_visualization()
        
        # æµ‹è¯•æŒ‡æ ‡æ”¶é›†å™¨
        test_metrics_collector()
        
        # æµ‹è¯•å®æ—¶å¯è§†åŒ–
        test_realtime_visualization()
        
        print("\nğŸ‰ æ‰€æœ‰å¯è§†åŒ–æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ“ æµ‹è¯•ç»“æœä¿å­˜åœ¨ test_plots/ å’Œ test_realtime/ ç›®å½•ä¸­")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
